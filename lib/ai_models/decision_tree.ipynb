{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Importing all the necessary libraries required for our Decision Tree AI\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.tree import export_text\r\n",
    "from sklearn.tree import export_graphviz\r\n",
    "from six import StringIO \r\n",
    "import csv\r\n",
    "from IPython.display import Image\r\n",
    "import pydotplus\r\n",
    "from termcolor import colored\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import json\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from nbconvert import PythonExporter\r\n",
    "from sklearn import metrics\r\n",
    "import firebase_admin\r\n",
    "from firebase_admin import credentials,storage\r\n",
    "from firebase_admin import firestore\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Initializing the firestore client to connect to our website database\r\n",
    "if not firebase_admin._apps:\r\n",
    "   cred = credentials.Certificate(\"services/we-don-t-byte---ass-firebase-adminsdk-kdbj6-ec500ebd6d.json\")\r\n",
    "   firebase_admin.initialize_app(cred, {\"storageBucket\": \"we-don-t-byte---ass.appspot.com\"})\r\n",
    "   db = firestore.client()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Before loading dataset we declare our column names to easily identify each file in our data.\r\n",
    "col_names = ['Prod_ID', 'Prod_Cat', 'User_ID', 'User_Province', 'Event', 'Clicks', 'Wishlist', 'Recommend']\r\n",
    "\r\n",
    "#loading our Dataset(To be slpit into Training and Validation Data)\r\n",
    "#Note: Test data will be fetched straight from the database, get compared to the training and validation we have to avoid testing \r\n",
    "#----- the model using the data it has seen already.\r\n",
    "\r\n",
    "pima = pd.read_csv(\"../../assets/decision_tree/decision_tree_data/Train_Data.csv\", header=None, names=col_names)\r\n",
    "\r\n",
    "#Setting our decision features(attributes to be used in our decision making---Decision Tree)\r\n",
    "feature_cols = ['Prod_Cat', 'User_Province', 'Clicks', 'Wishlist']\r\n",
    "\r\n",
    "#Setting our X and y variables for our Model -- Using our \"Recommend\" Column as our label - Since this is a decision tree(Supervised Learning)\r\n",
    "#and our feature Columns as our x \"terms\"\r\n",
    "X = pima[feature_cols]\r\n",
    "y = pima.Recommend\r\n",
    "\r\n",
    "#Here we split our dataset into training set and validation set(regarded as test in the following line)\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1) # 70% training and 30% validation\r\n",
    "\r\n",
    "#Note: Validation set will be used to Validate(test) our model and eventually calculate our Model accuracy before and after Pruning\r\n",
    "# --- but not to recommend...  \r\n",
    "\r\n",
    "#Here we declare our DecisionTreeClassifier as CLF and eventually train it using our train set and the criterion being \"entropy\"....\r\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\")\r\n",
    "clf = clf.fit(X_train,y_train)\r\n",
    "\r\n",
    "#We Create two json objects provinceJson and categoryJson to map our provinces and categories, respectively, to numerical values \r\n",
    "#that the model can be able to train and make neccessary calculations with\r\n",
    "provinceJson =  '{ \"Limpopo\" : 1,\"Gauteng\" : 2,\"Free State\" : 3,\"Western Cape\" : 4,\"KwaZulu-Natal\" : 5,\"North West\" : 6,\"Northern Cape\" : 7,\"Eastern Cape\" : 8,\"Mpumalanga\" : 9}'\r\n",
    "\r\n",
    "categoryJson =  '{\"Books\" : 1, \"Shoes\" : 2, \"Clothing\" : 3, \"Tech\" : 4, \"Kitchen\" : 5}'\r\n",
    "\r\n",
    "loadProvinceJ = json.loads(provinceJson)\r\n",
    "   \r\n",
    "loadCategoryJ = json.loads(categoryJson)\r\n",
    "\r\n",
    "#Here we keep track of all trained entries for later purposes...(Avoid testing the exact entries we've trained)\r\n",
    "# thus avoiding returning large lists as well......\r\n",
    "with open('../../assets/decision_tree/decision_tree_data/Train_Data.csv','r') as file1:\r\n",
    "    trainedEntries = [line for line in csv.reader(file1, delimiter=',')]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Here we are collecting all our products from the database (all documents in firebase)\r\n",
    "print('collecting Products... ')\r\n",
    "products_ref = db.collection('Products')\r\n",
    "Products_docs = products_ref.get()\r\n",
    "\r\n",
    "print('collecting User collection... ')\r\n",
    "#Collecting the user's collection from firebase\r\n",
    "users_ref = db.collection('Users')\r\n",
    "users_docs = users_ref.list_documents()\r\n",
    "print('All set!!!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Declare a list of recommendations (This is one of the final products of this Algorithm)\r\n",
    "recommended_products = []\r\n",
    "wishlistArr = []\r\n",
    "\r\n",
    "uid = '1RvEwuC2eCdzoKyaATXC9pNsXwH3'\r\n",
    "    \r\n",
    "province = ''\r\n",
    "    \r\n",
    "#Get User information--- \r\n",
    "userDoc1 = users_ref.document(uid)\r\n",
    "colDocs1 =  userDoc1.collection('info').get()\r\n",
    "colDocs2 = userDoc1.collection('Wishlist').list_documents()#Items in wishlist\r\n",
    "    \r\n",
    "#    Province\r\n",
    "#\r\n",
    "#Load the json value\r\n",
    "for colD_doc in colDocs1:\r\n",
    "    province = colD_doc.get('province')\r\n",
    "    province = loadProvinceJ[province]\r\n",
    "\r\n",
    "    \r\n",
    "#Note that here we're only recommending to users with a defined location, if province is null, then no recommendations\r\n",
    "# the continue statement goes to the next user (iteration)... else : we iterate through every product in the database running \r\n",
    "# it through our decision tree algorithm and getting a prediction of that and that's what we'll use for recommendations.. \r\n",
    "if province == '':\r\n",
    "    print(\"No recommendations for this user\")\r\n",
    "else:      \r\n",
    "    for x in colDocs2:\r\n",
    "        wishlistArr.append(x.id)\r\n",
    "\r\n",
    "        #For each product in the database :   \r\n",
    "    for prod_doc in Products_docs:  \r\n",
    "        pid = prod_doc.id  #product Id\r\n",
    "\r\n",
    "        s = pid in wishlistArr   #Returning true if the item is in the wishlist.....\r\n",
    "                ###\r\n",
    "                # Wishlist\r\n",
    "                ####\r\n",
    "        print(s)\r\n",
    "        if s :            \r\n",
    "            wishlist = 0\r\n",
    "            \r\n",
    "        else:\r\n",
    "            wishlist = 0   \r\n",
    "                \r\n",
    "        noOfClicks = 1    #To avoid null errors and a lot other unnecessary errors we give each product at least one click....\r\n",
    "            #\r\n",
    "            #   Clicks\r\n",
    "            #\r\n",
    "        if prod_doc.get('clicks')>= 1:\r\n",
    "            noOfClicks = prod_doc.get('clicks')\r\n",
    "            \r\n",
    "        category = prod_doc.get('category')\r\n",
    "        # print(\"Category: \" + str(loadCategoryJ[category]))\r\n",
    "        # print(\"Clicks: \" + str(noOfClicks))\r\n",
    "        category = loadCategoryJ[category]\r\n",
    "        \r\n",
    "        listTemp = [pid, category, uid, province, 'view', noOfClicks, wishlist] #Create test entry for this product to be run through the decision tree classifier....\r\n",
    "\r\n",
    "        if listTemp in trainedEntries : #Later purposes is now,lol, if this exact entery has been seen before in training, then don't test it\r\n",
    "            continue\r\n",
    "        else:\r\n",
    "                #creating a dataframe from our entry to be able to run through the classifier...\r\n",
    "            test_data = pd.DataFrame([listTemp], columns=['Prod_ID', 'Prod_Cat', 'User_ID', 'User_Province', 'Event', 'Clicks', 'Wishlist']) \r\n",
    "\r\n",
    "            y_predict = clf.predict(test_data[feature_cols]) #Get our 1(recommend) or 0(Do not recommend)\r\n",
    "\r\n",
    "            if(y_predict == 1):\r\n",
    "                str = pid    #Using a pipe delimeter\r\n",
    "                recommended_products.append(str) #appending to recommendation list\r\n",
    "\r\n",
    "                str = colored(pid, 'green')\r\n",
    "                print(str)\r\n",
    "                print(y_predict)\r\n",
    "            else:\r\n",
    "                str = colored(pid, 'red')\r\n",
    "                print(str)\r\n",
    "                print(y_predict)\r\n",
    "               \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#export our final list of recommendation as csv file to our assets folder in our project....\r\n",
    "np.savetxt(\"../../assets/decision_tree/decision_tree_outputs/final_recommendations.csv\", recommended_products, delimiter=\",\", fmt='%s')\r\n",
    "np.savetxt(\"../../assets/decision_tree/decision_tree_outputs/final_recommendations.txt\", recommended_products, delimiter=\",\", fmt='%s')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Performance testing using our validation data..... X_test was initialised as a validation set...\r\n",
    "\r\n",
    "#Before Pruning!!!!!!!!!!!!!!!!!!!!!!!!\r\n",
    "y_predict = clf.predict(X_test)\r\n",
    "\r\n",
    "#Get the accuracy of the model clf...\r\n",
    "\r\n",
    "AccuracyBeforePruning = metrics.accuracy_score(y_test, y_predict)\r\n",
    "print(\"-----------------------------------------------ACCURACY BEFORE PRUNING---------------------------------\\n\")\r\n",
    "print(colored(AccuracyBeforePruning, 'green'))\r\n",
    "print(\"\\n-------------------------------------------------------------------------------------------------------\\n\")\r\n",
    "\r\n",
    "\r\n",
    "#Building the decision tree, the visual decision tree diagram and exporting the diagram as .png and \r\n",
    "# saved under the assests folder in our project... \r\n",
    "r = export_text(clf, feature_names=feature_cols)\r\n",
    "\r\n",
    "print(colored(r, 'blue'))\r\n",
    "\r\n",
    "dot_data = StringIO()\r\n",
    "\r\n",
    "export_graphviz(clf, out_file=dot_data,  \r\n",
    "                filled=True, rounded=True,\r\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\r\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \r\n",
    "graph.write_png('../../assets/decision_tree/decision_tree_outputs/recommendations.png')\r\n",
    "Image(graph.create_png())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#After Pruning!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n",
    "#We prune the tree to reduce the complexity, makes it easier to interpret \r\n",
    "# by cutting off the redundency of some nodes of the tree... \r\n",
    "# \r\n",
    "#Sometimes there's a tradeoff between removing redundency and a slight decrease in model accuracy... A fair tradeoff....\r\n",
    "# what's good about our model is that most of the time the accuracy before and after pruning is likely the same, or even better after pruning\r\n",
    "\r\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4) #Given a depth of 4 to make the tree less complicated(Avoid overfitting)\r\n",
    "\r\n",
    "clf = clf.fit(X_train,y_train) #Train it again...\r\n",
    "\r\n",
    "y_pred = clf.predict(X_test)\r\n",
    "\r\n",
    "AccuracyAfterPruning = metrics.accuracy_score(y_test, y_pred)\r\n",
    "\r\n",
    "r = export_text(clf, feature_names=feature_cols)\r\n",
    "\r\n",
    "if(AccuracyBeforePruning <= AccuracyAfterPruning):\r\n",
    "    print(\"-----------------------------------------------ACCURACY AFTER PRUNING-------------------------------\\n\")\r\n",
    "    print(colored(AccuracyAfterPruning, 'green'))\r\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------------------\\n\")\r\n",
    "\r\n",
    "    print(colored(r, 'green'))\r\n",
    "else:\r\n",
    "    print(\"-----------------------------------------------ACCURACY AFTER PRUNING-------------------------------\\n\")\r\n",
    "    print(colored(AccuracyAfterPruning, 'red'))\r\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------------------\\n\")\r\n",
    "\r\n",
    "    print(colored(r, 'red'))\r\n",
    "\r\n",
    "dot_data = StringIO()\r\n",
    "\r\n",
    "export_graphviz(clf, out_file=dot_data,  \r\n",
    "                filled=True, rounded=True,\r\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\r\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \r\n",
    "graph.write_png('../../assets/decision_tree/decision_tree_outputs/recommendations(Pruned).png')\r\n",
    "Image(graph.create_png())\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Exporting and saving our accuracy file in a form Accuracy Before Pruning, Accuracy After Pruning\r\n",
    "accuracy = []\r\n",
    "accuracy.append(AccuracyBeforePruning)\r\n",
    "accuracy.append(AccuracyAfterPruning)\r\n",
    "np.savetxt(\"../../assets/DecisionTreeOutputs/accuracies.csv\", accuracy, delimiter=\",\", fmt='%s')\r\n",
    "\r\n",
    "#Exporting our final files to firebase (Note these are global files, files that do not depend on the user -- hence we didn't push the final _recommendations.csv file)\r\n",
    "#First we append them into an array...\r\n",
    "ai_results = []\r\n",
    "\r\n",
    "ai_results.append('DecisionTreeOutputs/recommendations.png')\r\n",
    "ai_results.append('DecisionTreeOutputs/recommendations(Pruned).png')\r\n",
    "ai_results.append('DecisionTreeOutputs/accuracies.csv')\r\n",
    "\r\n",
    "for x in ai_results:\r\n",
    "\r\n",
    "    bucket = storage.bucket()\r\n",
    "    blob = bucket.blob(x)\r\n",
    "    outfile='../../assets/' + x\r\n",
    "    with open(outfile, 'rb') as my_file:\r\n",
    "        blob.upload_from_file(my_file)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "a0db014c08daa843ea960079c032db7598c8fcf2c8f8e97c295474cf7df5bf03"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}